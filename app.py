import streamlit as st
import pandas as pd
from sklearn.linear_model import LogisticRegression

# Dataset simulado con y sin sesgo
def get_dataset(biased=True):
    if biased:
        # Modelo sesgado: rechaza sistemÃ¡ticamente ciertos perfiles
        data = {
            'income': [50000, 60000, 30000, 40000, 80000, 35000],
            'gender': [0, 1, 0, 1, 0, 1],  # 0: Masculino, 1: Femenino
            'race':   [0, 0, 1, 1, 0, 1],  # 0: Blanco, 1: Negro
            'approved': [1, 1, 0, 0, 1, 0]
        }
    else:
        # Modelo justo: decisiones basadas solo en ingresos, no en gÃ©nero/raza
        data = {
            'income': [50000, 60000, 30000, 40000, 80000, 35000],
            'gender': [0, 1, 0, 1, 0, 1],
            'race':   [0, 0, 1, 1, 0, 1],
            'approved': [1, 1, 0, 1, 1, 0]  # MÃ¡s balanceado y justo
        }
    return pd.DataFrame(data)

# Entrena un modelo simple
def train_model(biased=True):
    df = get_dataset(biased)
    X = df[['income', 'gender', 'race']]
    y = df['approved']
    model = LogisticRegression()
    model.fit(X, y)
    return model

# PredicciÃ³n
def predict(model, income, gender, race):
    X_new = pd.DataFrame([[income, gender, race]], columns=['income', 'gender', 'race'])
    return model.predict(X_new)[0]

# ==============================
# Interfaz de Streamlit
# ==============================

st.title("âš–ï¸ Â¿Y si el algoritmo es racista?")
st.markdown("### SimulaciÃ³n de sesgo en un modelo de aprobaciÃ³n de crÃ©dito")
st.markdown("Este ejemplo muestra cÃ³mo un modelo puede tomar decisiones injustas si los datos con los que fue entrenado contienen sesgos.")

# Entrada de usuario
income = st.number_input("ğŸ’µ Ingreso anual (USD)", min_value=10000, max_value=100000, step=5000)
gender = st.selectbox("ğŸ§ GÃ©nero", ["Masculino", "Femenino"])
race = st.selectbox("ğŸ§‘ Raza", ["Blanco", "Negro"])

# Convertir a valores numÃ©ricos
gender_val = 0 if gender == "Masculino" else 1
race_val = 0 if race == "Blanco" else 1

# Entrenar modelos
biased_model = train_model(biased=True)
fair_model = train_model(biased=False)

# Realizar predicciones
biased_result = predict(biased_model, income, gender_val, race_val)
fair_result = predict(fair_model, income, gender_val, race_val)

# Mostrar resultados
st.markdown("## ğŸ§ª Resultados:")
st.write(f"ğŸ”´ **Modelo con sesgo:** {'âœ… Aprobado' if biased_result else 'âŒ Rechazado'}")
st.write(f"ğŸŸ¢ **Modelo justo:** {'âœ… Aprobado' if fair_result else 'âŒ Rechazado'}")

# AnÃ¡lisis Ã©tico
if biased_result != fair_result:
    st.warning("âš ï¸ El modelo sesgado tomÃ³ una decisiÃ³n distinta influenciada por gÃ©nero o raza.")
else:
    st.success("âœ… Ambos modelos llegaron a la misma decisiÃ³n.")

# Detalles opcionales
with st.expander("ğŸ” Ver datos de entrenamiento"):
    st.markdown("#### Modelo Sesgado")
    st.dataframe(get_dataset(biased=True))
    st.markdown("#### Modelo Justo")
    st.dataframe(get_dataset(biased=False))
